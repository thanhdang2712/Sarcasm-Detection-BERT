{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTXqm3u_PR3T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# import emot\n",
        "# from emot.emo_unicode import UNICODE_EMOJI\n",
        "import re\n",
        "import csv\n",
        "dat_dir = './' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1nTLygePAQR"
      },
      "outputs": [],
      "source": [
        "## Not a complete data cleaning code because when manually going through the output data, some emoticons and emojis are not excluded (<10) and shouldn't affect much of the final result\n",
        "def data_preprocess(dat_dir,fname):\n",
        "  data = []\n",
        "  df = pd.read_excel(fname)\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if type(sent) == int:\n",
        "      df.drop(index,inplace=True)\n",
        "  ##Drop the tweet with emoji\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if 'ðŸ' in sent:\n",
        "      df.drop(index,inplace=True)\n",
        "  ##Manually recognized some characters included in the sentence to drop(some refers to emoticons)\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if '^' in sent:\n",
        "      df.drop(index,inplace=True)\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if 'â' in sent:\n",
        "      df.drop(index,inplace=True)\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if '&' in sent:\n",
        "      df.drop(index,inplace=True)\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if '-' in sent:\n",
        "      df.drop(index,inplace=True)\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if ':' in sent:\n",
        "      df.drop(index,inplace=True)\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if ')' in sent:\n",
        "      df.drop(index,inplace=True)\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if '¤' in sent:\n",
        "      df.drop(index,inplace=True)\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    if '(' in sent:\n",
        "      df.drop(index,inplace=True)\n",
        "\n",
        "      \n",
        "\n",
        "      \n",
        "\n",
        "  ##Remove hashtag and mention\n",
        "  for index, row in df.iterrows():\n",
        "    sent = row['sent']\n",
        "    clean_tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", sent)\n",
        "    clean_tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", clean_tweet)\n",
        "    data.append({'label': row['label'], 'sent':clean_tweet.replace(u'\\xa0', u'')})\n",
        "  \n",
        "  \n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summary(data):\n",
        "  sarcastic = 0\n",
        "  non_sarcastic = 0\n",
        "  for dic in data:\n",
        "    if dic['label'] == 0:\n",
        "      non_sarcastic +=1\n",
        "    else:\n",
        "      sarcastic +=1 \n",
        "  print(\"The final data set includes\",sarcastic,\"sarcastic sentences and\",non_sarcastic,\"non-sarcastic sentences.\")\n",
        "  "
      ],
      "metadata": {
        "id": "Q9rPJ9YquM_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niX0af7FPVAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e51f633-52e6-4fda-a1e4-81bd3c3d552c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The final data set includes 692 sarcastic sentences and 754 non-sarcastic sentences.\n"
          ]
        }
      ],
      "source": [
        "tweet_data = data_preprocess(dat_dir,'tweets_data.xlsx')\n",
        "fields = ['label', 'sent'] \n",
        "summary(tweet_data)\n",
        "with open('clean_tweet.csv', 'w', newline='') as file: \n",
        "    writer = csv.DictWriter(file, fieldnames = fields)\n",
        "    writer.writeheader() \n",
        "    writer.writerows(tweet_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOaRJAwfR_FC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b05bad-f7af-46e6-829e-738e05f3c3c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The final data set includes 680 sarcastic sentences and 651 non-sarcastic sentences.\n"
          ]
        }
      ],
      "source": [
        "reddit_data = data_preprocess(dat_dir,'reddit_data.xlsx')\n",
        "to_remove = 1384-685\n",
        "start = 0 \n",
        "while to_remove>=0:\n",
        "  dic = reddit_data[start]\n",
        "  if dic['label'] ==0:\n",
        "    del reddit_data[start]\n",
        "    to_remove -= 1\n",
        "  start+=1\n",
        "\n",
        "summary(reddit_data)\n",
        "fields = ['label', 'sent'] \n",
        "with open('clean_reddit.csv', 'w', newline='') as file: \n",
        "    writer = csv.DictWriter(file, fieldnames = fields)\n",
        "    writer.writeheader() \n",
        "    writer.writerows(tweet_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_G7ah4idpYSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}