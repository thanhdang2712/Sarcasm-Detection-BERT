{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEhz-8RNwg1H"
   },
   "source": [
    "# Sarcasm Detection Baseline with Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "vocab_file = 'glove_vocab.txt'\n",
    "\n",
    "vocab = set()\n",
    "with open(vocab_file) as f:\n",
    "  for line in f:\n",
    "    if line in vocab:\n",
    "      print(line, 'is repeated')\n",
    "    vocab.add(line.strip())\n",
    "\n",
    "print(len(vocab))\n",
    "## Add in UNK to the vocab\n",
    "vocab.add('[UNK]')\n",
    "\n",
    "## Check if contractions are in the vocab, if not add\n",
    "contractions = [\"n't\", \"'ll\", \"'re\", \"'s\"]\n",
    "for word in contractions:\n",
    "  if word not in vocab:\n",
    "    print('Adding', word)\n",
    "    vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_run(k, ngram, X_train, Y_train):\n",
    "    '''\n",
    "    Run a Multinomial NB classifier with Laplace smoothing, using normal frequency count.\n",
    "    Parameters:\n",
    "        - k (int): the k value for addk smoothing\n",
    "        - ngram (tuple of int, int): length of n-gram word window\n",
    "    '''\n",
    "    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "    cv = CountVectorizer(stop_words='english', ngram_range = ngram, tokenizer = token.tokenize, vocabulary=vocab)\n",
    "    text_counts = cv.fit_transform(X_train)      \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(text_counts, Y_train,test_size=0.1, random_state=5)\n",
    "    MNB = MultinomialNB(alpha=k)\n",
    "    MNB.fit(X_train, Y_train)\n",
    "    score(MNB, X_test, Y_test)\n",
    "    return MNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_run(k, ngram, X_train, Y_train):\n",
    "    '''\n",
    "    Run a Multinomial NB classifier with Laplace smoothing, using TF-IDF.\n",
    "    Parameters:\n",
    "        - k (int): the k value for addk smoothing\n",
    "        - ngram (tuple of int, int): length of n-gram word window\n",
    "    '''\n",
    "    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "    tfidf = TfidfVectorizer(stop_words='english', tokenizer = token.tokenize, vocabulary=vocab)\n",
    "    text_counts = tfidf.fit_transform( X_train)     \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(text_counts, Y_train,test_size=0.1, random_state=5)\n",
    "    MNB = MultinomialNB(alpha=k)      #defining the model\n",
    "    MNB.fit(X_train, Y_train)         #compilimg the model\n",
    "    score(MNB, X_test, Y_test)                        # evaluate with accuracy and F-1 score\n",
    "    return MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5rTFJF3wg1P",
    "outputId": "40608268-83f8-49f9-a131-6dc2eff6ba05"
   },
   "outputs": [],
   "source": [
    "def score(model, X_test, Y_test):\n",
    "    '''\n",
    "    Report accuracy and f1-score.\n",
    "    '''\n",
    "    predicted = model.predict(X_test)\n",
    "    print(predicted)\n",
    "    accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "    print(str('Accuracy: {:04.2f}'.format(accuracy_score)))\n",
    "    precision_score = metrics.precision_score(predicted, Y_test)\n",
    "    print(str('Precision: {:04.2f}'.format(precision_score)))\n",
    "    recall_score = metrics.recall_score(predicted, Y_test)\n",
    "    print(str('Recall: {:04.2f}'.format(recall_score)))\n",
    "    f1_score = metrics.f1_score(predicted, Y_test)\n",
    "    print(str('F-1 score: {:04.2f}'.format(f1_score)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Alc2T9SOwl-G"
   },
   "outputs": [],
   "source": [
    "sarcasm_df = pd.read_json(\"sarcasm.json\", lines=True)\n",
    "sarcasm_df = sarcasm_df.drop(['article_link'], axis=1)\n",
    "sarcasm_X = list(sarcasm_df.headline)\n",
    "sarcasm_Y = (sarcasm_df.is_sarcastic)\n",
    "sarcasm_test_sent = sarcasm_X[int(0.9*len(sarcasm_X)):]\n",
    "sarcasm_X_train = sarcasm_X[:int(0.9*len(sarcasm_X))]\n",
    "sarcasm_test_labels = sarcasm_Y[int(0.9*len(sarcasm_Y)):]\n",
    "sarcasm_Y_train = sarcasm_Y[:int(0.9*len(sarcasm_X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F-1 score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# TF Baseline for Headlines: Hyperparameter tuning\n",
    "m1 = normal_run(10, (3,5), sarcasm_X_train, sarcasm_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F-1 score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Testing on Test set\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english', ngram_range = (5,5), tokenizer = token.tokenize, vocabulary=vocab)\n",
    "text_counts = cv.fit_transform(sarcasm_test_sent)      \n",
    "score(m1, text_counts, sarcasm_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Precision: 0.61\n",
      "Recall: 0.87\n",
      "F-1 score: 0.72\n"
     ]
    }
   ],
   "source": [
    "M2 = tfidf_run(10, (5,5),sarcasm_X_train, sarcasm_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Precision: 0.59\n",
      "Recall: 0.86\n",
      "F-1 score: 0.70\n"
     ]
    }
   ],
   "source": [
    "# Testing on Headline test set\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = TfidfVectorizer(stop_words='english', ngram_range = (5,5), tokenizer = token.tokenize, vocabulary=vocab)\n",
    "text_counts = cv.fit_transform(sarcasm_test_sent)      \n",
    "score(M2, text_counts, sarcasm_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1331 entries, 0 to 1330\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   1331 non-null   int64 \n",
      " 1   sent    1331 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 20.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    680\n",
       "0    651\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_reddit.csv')\n",
    "reddit_sent = df.sent.values\n",
    "reddit_label = df.label.values\n",
    "df.info()\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Accuracy: 0.49\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F-1 score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Accuracy: 0.49\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F-1 score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Testing TF on Reddit set\n",
    "cv = CountVectorizer(stop_words='english', ngram_range = (1,5), tokenizer = token.tokenize, vocabulary=vocab, min_df=0.02)\n",
    "text_counts = cv.fit_transform(reddit_sent)      \n",
    "score(m1, text_counts, reddit_label)\n",
    "\n",
    "# Testing IDF on Reddit set\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = TfidfVectorizer(stop_words='english', ngram_range = (5,5), tokenizer = token.tokenize, vocabulary=vocab)\n",
    "text_counts = cv.fit_transform(reddit_sent)      \n",
    "score(M2, text_counts, reddit_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1446 entries, 0 to 1445\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   1446 non-null   int64 \n",
      " 1   sent    1446 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 22.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    754\n",
       "1    692\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_tweet.csv')\n",
    "tweet_sent = df.sent.values\n",
    "tweet_label = df.label.values\n",
    "df.info()\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Accuracy: 0.52\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F-1 score: 0.00\n",
      "[0 1 0 ... 1 0 0]\n",
      "Accuracy: 0.55\n",
      "Precision: 0.31\n",
      "Recall: 0.55\n",
      "F-1 score: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhdang/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Testing TF on twitter set\n",
    "cv = CountVectorizer(stop_words='english', ngram_range = (2,5), tokenizer = token.tokenize, vocabulary=vocab)\n",
    "text_counts = cv.fit_transform(tweet_sent)      \n",
    "score(m1, text_counts, tweet_label)\n",
    "\n",
    "# Testing IDF on twitter set\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = TfidfVectorizer(stop_words='english', ngram_range = (1,5), tokenizer = token.tokenize, vocabulary=vocab)\n",
    "text_counts = cv.fit_transform(tweet_sent)      \n",
    "score(M2, text_counts, tweet_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2bwQkSpwg1N"
   },
   "source": [
    "Now we have the training and testing data. We should start the analysis. Our analysis (as most of ML analysis) will be in 5 steps(a mneumonic to remember them is <b>DC-FEM</b> remember as DC Female or District of Columbia Fire and Emergency Medical service): \n",
    "<ol>\n",
    "    <li>Defining the model</li>\n",
    "    <li>Compiling the model</li>\n",
    "    <li>Fitting the model</li>\n",
    "    <li>Evaluating the model</li>\n",
    "    <li>Making predictions with the model</li>\n",
    "</ol>\n",
    " \n",
    "### 1. Defining the model\n",
    "We will use one of the __[Naive Bayes (NB)](https://scikit-learn.org/stable/modules/naive_bayes.html)__ classifier for defining the model. Specifically we will use __[MultinomialNB classifier](https://scikit-learn.org/stable/modules/naive_bayes.html)__. As a fresher to ML one can use cheat sheet given by sklearn __[here](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)__ to determine the best model to use for a particular problem. It tell us to use NB classifier. Let us take a detour to learn more about NB model. \n",
    "####  Naive Bayes Model\n",
    "This model applies Bayes theorem with a Naive assumption of no relation between different features. According to Bayes theorem:<br>\n",
    "\n",
    "Posterior = likelihood * proposition/evidedence or  P(A|B) = P(B|A) * P(A)/P(B)<br>\n",
    "<b>For ex: In a deck of playing cards, a card is chosen. What is the probability of a card being queen given the card is a face card?</b><br>\n",
    "This can be solved using Bayes theorem.<br>\n",
    "P(Queen given Face card) = P(Queen|Face)<br> \n",
    "P(Face given Queen) = P(Face|Queen) = 1<br>\n",
    "P(Queen) = 4/52 = 1/13\n",
    "P(Face) = 3/13\n",
    "From Bayes theore:<br>\n",
    "P(Queen|Face) = P(Face|Queen) P(Queen)/P(Face) = 1/3<br>\n",
    "\n",
    "\n",
    "\n",
    "For an input with several variables:<br>\n",
    "P(y|x1, x2, ... xn) = P(x1, x2, ... xn|y)* P(y)/P(x1,x2, ...xn)<br>\n",
    "with Naive Bayes we assume x1, x2 ... xn are independent of each other, i.e:<br>\n",
    "P(x1, x2, ... xn|y) = P(x1|y) * P(x2|y) ... * P(xn|y)<br> \n",
    "The assumption in distribution of P(xi|y) give rise to different NBM. For example assuming Gaussian distribution will give rise to Gaussian Naive Bayes (GNB) or multinomial distribusion will give Multinomial Naive Bayes (MNB). \n",
    "\n",
    "Naive Bayes Model works particularly well with text classification and spam filtering. <b>Advantages</b> of working with NB algorithm are:\n",
    "<ul>\n",
    "    <li>Requires small amount of training data to learn the parameters</li>\n",
    "    <li>Can be trained relatively fast compared to sophisticated models</li>\n",
    "</ul>\n",
    "Main <b>disadvantage</b> of NB Algorithm is:\n",
    "<ul>\n",
    "    <li>It's a decent classifier but a bad estimator</li>\n",
    "    <li>It works well with discrete values but won't work with continuous values (can't be used in regression)</li>\n",
    "</ul>\n",
    "\n",
    "#### Dilemma of NB Algorithm\n",
    "A challenging question which can be asked regarding NB algorithm is: although the condinal independence assumed in NB algorithm is hardly true in real life then howcome NB Algorithm work so well as classifier? \n",
    "I won't discuss the solution here, rather will direct you towards the resource which contains the solution (__[here](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)__). In short the answer lies in distribution of dependencies rather than dependency, somehow due to distribution the effect of dependencies cancels out. \n",
    "\n",
    "#### Loss function for NB classification\n",
    "NB classification uses a zero-one loss function. In this function error = number of incorrect classifications. Here accuracy of probability estimation is not taken into account by error function given that class with highest probability is predicted right. For example let's say there are two classes A and B, and different attributes (x1, x2, ... xn) are given. P(A|all atributes) = 0.95 and P(B|all atributes)=0.05 but NB might estimates P(A|all atributes) = 0.7 and P(B|all atributes) = 0.3. Here althogh estimates are far from accurate but classifiction is correct.\n",
    "\n",
    "Let's move back to our analysis. The first two steps of defining and compiling the model are reduced to identifying and importing the model from sklearn (as sklearn gives as precompiled models).\n",
    "\n",
    "### 2. Compiling the model\n",
    "Since we are using sklearn's modules and classes we just need to import the precompiled classes. Sklearn gives the information of all the classes __[here](https://scikit-learn.org/stable/modules/classes.html)__.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjYmbYimwg1O"
   },
   "source": [
    "### 3. Fitting the model\n",
    "In this step we generate our model fitting our dataset in the MultinomialNB. Inorder to look for the arguments which can be passed while fitting the model its advised to check the sklearn webpage of the module under use. For MNB it can be checked __[here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRLnsY8Jwg1O"
   },
   "source": [
    "### 4. Evaluating the model\n",
    "Here we quantify the quality of our model. We use __[metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)__ module from sklearn library to evaluate the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0mOYkvgwg1P"
   },
   "source": [
    "## Tweaking the model\n",
    "We have observed the accuracy of our model is over 60%. We can now play with our model to increase its' accuracy.\n",
    "\n",
    "#### Trying different n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkU50T9Vwg1P"
   },
   "source": [
    "#### Trying different Naive Bayes Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piIn0H-qwg1Q"
   },
   "source": [
    "How about using several different algorithms all at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ1PPTT6wg1Q"
   },
   "source": [
    "### Improving the accuracy\n",
    "We have tried using different n-grams and different Naive Bayes models but maximum accuracy lingers arround 60%. In order to improve our model let's try to change the way the BOW is created. Currently we created BOW with CountVectorizer which counts the occurance of the word in the text. More number of time a word occurs it becomes more important for classification. \n",
    "\n",
    "\n",
    "### TF-IDF: Term Frequency-Inverse Document Frequency\n",
    "Let's use TF-IDF here product of term frequency and inverse document frequency is used. Term frequency is how frequently a terms has appeared in a document. Let's say a term appears f times in a document with d words. <br>\n",
    "Term Frequency = f/d <br>\n",
    "IDF is inverse document frequency. If a corpus contains N documents and the term of our interest appears only in D documents then IDF is:<br>\n",
    "IDF = log(N/D)\n",
    "TF-IDF is product of Term Frequncy and Inverse Document Frequency. <b>TF-IDF shows the rarity of a word in the corpus.</b> If a word is rare then probably its a signature word for a particular sentiment/information.  \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
