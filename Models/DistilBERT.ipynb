{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sq6Fphct_x7F"
   },
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8urEVcpfO1A",
    "outputId": "7ee34167-1b3a-4544-87be-3e14a3c2188e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /opt/tljh/user/lib/python3.9/site-packages (4.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UlxtbvkcHJon"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 12:55:22.594701: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-03 12:55:22.657680: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-03 12:55:22.658628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 12:55:23.895895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import TFBertModel, BertTokenizer, DistilBertTokenizer, TFDistilBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "tfxmsktEK2lO",
    "outputId": "60506e35-49a5-49e6-c0a3-0f4136d8936b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('sarcasm.json',lines=True)\n",
    "labels = data.is_sarcastic.values\n",
    "sentences = data.headline.values\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdJoKlIIdrux"
   },
   "source": [
    "# DistillBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tT0ZKsSfJkrc"
   },
   "outputs": [],
   "source": [
    "train_sents,test_sents, train_labels, test_labels  = train_test_split(sentences,labels,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dip8f0WBdrux",
    "outputId": "bd8a70ce-b11e-4b23-84e1-4dc6b7fd6500"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,do_lower_case = True)\n",
    "\n",
    "def encoder_dis(sentences):\n",
    "  ids = []\n",
    "  for sentence in sentences:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "    sentence,\n",
    "    max_length=16,\n",
    "    truncation = True,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=False)\n",
    "    ids.append(encoding['input_ids'])\n",
    "  return ids\n",
    "\n",
    "train_ids_dis = encoder_dis(train_sents)\n",
    "test_ids_dis = encoder_dis(test_sents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SICB4Wl9drux"
   },
   "outputs": [],
   "source": [
    "train_ids_dis = tf.convert_to_tensor(train_ids_dis)\n",
    "test_ids_dis = tf.convert_to_tensor(test_ids_dis)\n",
    "test_labels_dis = tf.convert_to_tensor(test_labels)\n",
    "train_labels_dis = tf.convert_to_tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xu_FffTgdruy",
    "outputId": "0e5f1b77-a3c2-4d5d-fb6a-71487c3988c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "distillbert_encoder = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "input_word_ids = tf.keras.Input(shape=(16,), dtype=tf.int32, name=\"input_word_ids\")  \n",
    "embedding = distillbert_encoder([input_word_ids])\n",
    "dense = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(embedding[0])\n",
    "dense = tf.keras.layers.Dense(128, activation='relu')(dense)\n",
    "dense = tf.keras.layers.Dropout(0.2)(dense)   \n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)    \n",
    "\n",
    "model_dis = tf.keras.Model(inputs=[input_word_ids], outputs=output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bfOn7lI5druy"
   },
   "outputs": [],
   "source": [
    "model_dis.compile(tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy', 'f1score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwEajDhFdruy",
    "outputId": "e9802f5b-4c25-4037-a455-ba0a09f1ac56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "725/725 [==============================] - 405s 547ms/step - loss: 0.4301 - accuracy: 0.7910 - val_loss: 0.3279 - val_accuracy: 0.8579\n",
      "Epoch 2/3\n",
      "725/725 [==============================] - 392s 541ms/step - loss: 0.2840 - accuracy: 0.8796 - val_loss: 0.3121 - val_accuracy: 0.8700\n",
      "Epoch 3/3\n",
      "725/725 [==============================] - 392s 541ms/step - loss: 0.1996 - accuracy: 0.9188 - val_loss: 0.3016 - val_accuracy: 0.8804\n"
     ]
    }
   ],
   "source": [
    "trained_dis = model_dis.fit(x = train_ids_dis, y = train_labels_dis, epochs = 3, verbose = 1, batch_size = 32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-distribution Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 11s 120ms/step\n",
      "accuracy 0.8784067085953878\n",
      "precision 0.879154078549849\n",
      "recall 0.8609467455621301\n",
      "f1 0.8699551569506726\n"
     ]
    }
   ],
   "source": [
    "predictions_indis = [1 * (x[0]>=0.5) for x in model_dis.predict(test_ids_dis)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis, predictions_indis)\n",
    "precision = metrics.precision_score(test_labels_dis, predictions_indis)\n",
    "recall = metrics.recall_score(test_labels_dis, predictions_indis)\n",
    "f1 = metrics.f1_score(test_labels_dis, predictions_indis)\n",
    "\n",
    "print(\"accuracy\",accuracy)\n",
    "print(\"precision\",precision)\n",
    "print(\"recall\",recall)\n",
    "print(\"f1\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6MEdQdpq6uW",
    "outputId": "ddae1c7f-2175-424d-b46f-7d5eeac7e4b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#generate ids for reddit test data\n",
    "test_data1 = pd.read_csv('clean_reddit.csv')\n",
    "test_labels1 = test_data1.label.values\n",
    "test_sentences1 = test_data1.sent.values\n",
    "test_data1.head()\n",
    "\n",
    "\n",
    "test_ids1 = encoder_dis(test_sentences1)\n",
    "test_ids1 = tf.convert_to_tensor(test_ids1)\n",
    "test_labels_dis1 = tf.convert_to_tensor(test_labels1)\n",
    "# print(\"first 5 test_ids1: \", test_ids1[:5])\n",
    "# print(\"first 5 test_labels_dis1: \", test_labels_dis1[:5])\n",
    "\n",
    "\n",
    "#generate ids for tweet test data\n",
    "test_data2 = pd.read_csv('clean_tweet.csv')\n",
    "test_labels2 = test_data2.label.values\n",
    "test_sentences2 = test_data2.sent.values\n",
    "test_data2.head()\n",
    "\n",
    "\n",
    "test_ids2 = encoder_dis(test_sentences2)\n",
    "test_ids2 = tf.convert_to_tensor(test_ids2)\n",
    "test_labels_dis2 = tf.convert_to_tensor(test_labels2)\n",
    "# print(\"first 5 test_ids2: \", test_ids2[:5])\n",
    "# print(\"first 5 test_labels_dis2: \", test_labels_dis2[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-distribution Tests on Reddits and Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 6s 120ms/step\n",
      "accuracy 0.5152143845089903\n",
      "precision 0.44155844155844154\n",
      "recall 0.049132947976878616\n",
      "f1 0.08842652795838751\n"
     ]
    }
   ],
   "source": [
    "predictions_tweets = [1 * (x[0]>=0.5) for x in model_dis.predict(test_ids2)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis2, predictions_tweets)\n",
    "precision = metrics.precision_score(test_labels_dis2, predictions_tweets)\n",
    "recall = metrics.recall_score(test_labels_dis2, predictions_tweets)\n",
    "f1 = metrics.f1_score(test_labels_dis2, predictions_tweets)\n",
    "\n",
    "print(\"accuracy\",accuracy)\n",
    "print(\"precision\",precision)\n",
    "print(\"recall\",recall)\n",
    "print(\"f1\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Baseline Testing (Untrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_transform', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "distillbert_encoder = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "input_word_ids = tf.keras.Input(shape=(16,), dtype=tf.int32, name=\"input_word_ids\")  \n",
    "embedding = distillbert_encoder([input_word_ids])\n",
    "dense = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(embedding[0])\n",
    "dense = tf.keras.layers.Dense(128, activation='relu')(dense)\n",
    "dense = tf.keras.layers.Dropout(0.2)(dense)   \n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)    \n",
    "\n",
    "model_base = tf.keras.Model(inputs=[input_word_ids], outputs=output)  \n",
    "model_base.compile(tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 5s 121ms/step\n",
      "accuracy 0.5093914350112697\n",
      "precision 0.510158013544018\n",
      "recall 0.9970588235294118\n",
      "f1 0.6749626679940268\n"
     ]
    }
   ],
   "source": [
    "predictions_reddit_base = [1 * (x[0]>=0.5) for x in model_base.predict(test_ids1)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis1, predictions_reddit_base)\n",
    "precision = metrics.precision_score(test_labels_dis1, predictions_reddit_base)\n",
    "recall = metrics.recall_score(test_labels_dis1, predictions_reddit_base)\n",
    "f1 = metrics.f1_score(test_labels_dis1, predictions_reddit_base)\n",
    "print(\"accuracy\",accuracy)\n",
    "print(\"precision\",precision)\n",
    "print(\"recall\",recall)\n",
    "print(\"f1\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 5s 118ms/step\n",
      "accuracy 0.4778699861687414\n",
      "precision 0.4782006920415225\n",
      "recall 0.9985549132947977\n",
      "f1 0.6467009826860084\n"
     ]
    }
   ],
   "source": [
    "predictions_tweet_base = [1 * (x[0]>=0.5) for x in model_base.predict(test_ids2)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis2, predictions_tweet_base)\n",
    "precision = metrics.precision_score(test_labels_dis2, predictions_tweet_base)\n",
    "recall = metrics.recall_score(test_labels_dis2, predictions_tweet_base)\n",
    "f1 = metrics.f1_score(test_labels_dis2, predictions_tweet_base)\n",
    "print(\"accuracy\",accuracy)\n",
    "print(\"precision\",precision)\n",
    "print(\"recall\",recall)\n",
    "print(\"f1\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 11s 120ms/step\n",
      "accuracy 0.4744933612858141\n",
      "precision 0.47335203366058903\n",
      "recall 0.9985207100591716\n",
      "f1 0.6422454804947668\n"
     ]
    }
   ],
   "source": [
    "predictions_news = [1 * (x[0]>=0.5) for x in model_base.predict(test_ids_dis)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis, predictions_news)\n",
    "precision = metrics.precision_score(test_labels_dis, predictions_news)\n",
    "recall = metrics.recall_score(test_labels_dis, predictions_news)\n",
    "f1 = metrics.f1_score(test_labels_dis, predictions_news)\n",
    "print(\"accuracy\",accuracy)\n",
    "print(\"precision\",precision)\n",
    "print(\"recall\",recall)\n",
    "print(\"f1\", f1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
